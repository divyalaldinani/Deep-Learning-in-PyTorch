{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBdEPn7NAt4jVno6v4gO8a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyalaldinani/Deep-Learning-in-PyTorch/blob/main/RNN_in_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "8aaqB5IhVOfO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "jyXN6h7AV1jA"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(r\"/content/train.csv\", dtype=np.float32)\n",
        "labels = train.label.values\n",
        "\n",
        "image_data = train.loc[:, train.columns != 'label'].values/255\n",
        "image_data.min(), image_data.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrwKcMTDbiwr",
        "outputId": "06552558-e9e8-4c10-ab78-c3a2a9cce4ed"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_data.shape # 784 -> pixels in an image, 42000 -> number of images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v93f2C9ceFW",
        "outputId": "932d8bf6-460b-4c58-b358-39577b9ebbe6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(image_data, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "6iaj35L0cqIA"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_torch = torch.from_numpy(train_data)\n",
        "test_data_torch = torch.from_numpy(test_data)\n",
        "\n",
        "train_labels_torch = torch.from_numpy(train_labels).type(torch.LongTensor)\n",
        "test_labels_torch = torch.from_numpy(test_labels).type(torch.LongTensor)"
      ],
      "metadata": {
        "id": "g2PlnPSZdTEf"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#settings hyperparameters\n",
        "\n",
        "batch_size = 100\n",
        "iters = 20000\n",
        "epochs = iters / (len(train_data)/batch_size)\n",
        "epochs = int(epochs)\n",
        "## Note that epochs = number of times the train data is traversed so that all samples are covered, in each epoch, only batch_size number of samples are trained on"
      ],
      "metadata": {
        "id": "t8P_mu-nd9MH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(train_data_torch, train_labels_torch)\n",
        "test_dataset = TensorDataset(test_data_torch, test_labels_torch)"
      ],
      "metadata": {
        "id": "EtpC3_6IeUUA"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size= batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "nudto0NSfNG7"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(train_data[80].reshape(28, 28))\n",
        "plt.axis(False)\n",
        "plt.title(str(train_labels[80]))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "QATiAQaGfbWf",
        "outputId": "2776ce1a-3a7d-48d7-900d-35cc32534e20"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 hidden layer\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, hidden_layers, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.hidden_layers = hidden_layers\n",
        "\n",
        "\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, hidden_layers, batch_first=True, nonlinearity='relu')\n",
        "\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.hidden_layers, x.size(0), self.hidden_size)\n",
        "\n",
        "    out, hn = self.rnn(x, h0) # out -> contains hidden states of all time stepsfrom the last layer\n",
        "\n",
        "    out = self.fc(out[:, -1, :]) # out[:, -1, :] -> last hidden layer of seq, out now denotes the output of fc layer by inputtting the output of RNN\n",
        "    return out"
      ],
      "metadata": {
        "id": "8HzBOky7fySI"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 28\n",
        "hidden_size = 100\n",
        "hidden_layers = 1\n",
        "output_size = 10 # 10 numbers\n",
        "\n",
        "model = RNN(input_size, hidden_size, hidden_layers, output_size)\n",
        "\n",
        "error = nn.CrossEntropyLoss()\n",
        "lr = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "id": "OokkXFmRl3Fx"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "accuracy = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    count = 0;\n",
        "    for i, (images, labels ) in enumerate(train_loader):\n",
        "        train = images.view(-1, input_size, input_size)\n",
        "        # labels = labels\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(train)\n",
        "\n",
        "        loss = error(outputs, labels)\n",
        "        loss.backward() #backward prop\n",
        "\n",
        "        optimizer.step() # update params\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        if count % 100 == 0 :\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for images, labels in test_loader:\n",
        "                images = images.view(-1, input_size, input_size)\n",
        "\n",
        "                outputs = model(images)\n",
        "                predictions = torch.max(outputs.data, 1)[1] # .data -> access values, outputs -> (batch_size, num_classes)\n",
        "\n",
        "                total += images.size(0)\n",
        "                correct += (predictions == labels).sum()\n",
        "\n",
        "            acc = 100*float(correct/total)\n",
        "\n",
        "            accuracy.append(acc)\n",
        "            losses.append(loss.data)\n",
        "\n",
        "        if count % 300 == 0:\n",
        "            print(f\"Iteration: {count}, Loss: {loss.data}, Accuracy: {acc}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PrttJlNmlka",
        "outputId": "99c222c2-03e8-46d0-8ee6-38bf9a22bd05"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 300, Loss: 1.2707085609436035, Accuracy: 51.249998807907104\n",
            "Iteration: 300, Loss: 1.051285743713379, Accuracy: 62.03571557998657\n",
            "Iteration: 300, Loss: 0.9402324557304382, Accuracy: 66.54762029647827\n",
            "Iteration: 300, Loss: 0.8690007925033569, Accuracy: 70.79761624336243\n",
            "Iteration: 300, Loss: 0.7963180541992188, Accuracy: 74.48809742927551\n",
            "Iteration: 300, Loss: 0.7510060667991638, Accuracy: 76.5238106250763\n",
            "Iteration: 300, Loss: 0.7072566151618958, Accuracy: 78.15476059913635\n",
            "Iteration: 300, Loss: 0.6571023464202881, Accuracy: 80.48809766769409\n",
            "Iteration: 300, Loss: 0.6064454913139343, Accuracy: 82.03571438789368\n",
            "Iteration: 300, Loss: 0.5704972147941589, Accuracy: 83.29761624336243\n",
            "Iteration: 300, Loss: 0.5371761918067932, Accuracy: 84.28571224212646\n",
            "Iteration: 300, Loss: 0.5250203609466553, Accuracy: 84.96428728103638\n",
            "Iteration: 300, Loss: 0.5058622360229492, Accuracy: 85.69047451019287\n",
            "Iteration: 300, Loss: 0.4813905954360962, Accuracy: 86.28571629524231\n",
            "Iteration: 300, Loss: 0.46214616298675537, Accuracy: 86.73809766769409\n",
            "Iteration: 300, Loss: 0.44686734676361084, Accuracy: 87.285715341568\n",
            "Iteration: 300, Loss: 0.4305793344974518, Accuracy: 87.63095140457153\n",
            "Iteration: 300, Loss: 0.4145583212375641, Accuracy: 88.03571462631226\n",
            "Iteration: 300, Loss: 0.400839626789093, Accuracy: 88.17856907844543\n",
            "Iteration: 300, Loss: 0.3859933912754059, Accuracy: 88.46428394317627\n",
            "Iteration: 300, Loss: 0.37406250834465027, Accuracy: 88.5357141494751\n",
            "Iteration: 300, Loss: 0.3643817603588104, Accuracy: 88.67856860160828\n",
            "Iteration: 300, Loss: 0.35463130474090576, Accuracy: 88.99999856948853\n",
            "Iteration: 300, Loss: 0.34470173716545105, Accuracy: 88.8452410697937\n",
            "Iteration: 300, Loss: 0.3343507647514343, Accuracy: 89.27381038665771\n",
            "Iteration: 300, Loss: 0.3253955543041229, Accuracy: 89.58333134651184\n",
            "Iteration: 300, Loss: 0.314203143119812, Accuracy: 89.9404764175415\n",
            "Iteration: 300, Loss: 0.3069823682308197, Accuracy: 90.10714292526245\n",
            "Iteration: 300, Loss: 0.3001413941383362, Accuracy: 90.24999737739563\n",
            "Iteration: 300, Loss: 0.293129026889801, Accuracy: 90.30952453613281\n",
            "Iteration: 300, Loss: 0.28781363368034363, Accuracy: 90.29762148857117\n",
            "Iteration: 300, Loss: 0.2841590940952301, Accuracy: 90.36904573440552\n",
            "Iteration: 300, Loss: 0.2789575457572937, Accuracy: 90.51190614700317\n",
            "Iteration: 300, Loss: 0.27337518334388733, Accuracy: 90.54762125015259\n",
            "Iteration: 300, Loss: 0.2701306641101837, Accuracy: 90.6428575515747\n",
            "Iteration: 300, Loss: 0.26688632369041443, Accuracy: 90.80952405929565\n",
            "Iteration: 300, Loss: 0.2617841064929962, Accuracy: 90.9761905670166\n",
            "Iteration: 300, Loss: 0.2579384744167328, Accuracy: 91.22619032859802\n",
            "Iteration: 300, Loss: 0.2567126452922821, Accuracy: 91.34523868560791\n",
            "Iteration: 300, Loss: 0.2568972706794739, Accuracy: 91.3690447807312\n",
            "Iteration: 300, Loss: 0.2546544373035431, Accuracy: 91.40475988388062\n",
            "Iteration: 300, Loss: 0.2529962360858917, Accuracy: 91.45237803459167\n",
            "Iteration: 300, Loss: 0.2543007433414459, Accuracy: 91.65475964546204\n",
            "Iteration: 300, Loss: 0.2517791986465454, Accuracy: 91.78571701049805\n",
            "Iteration: 300, Loss: 0.24969106912612915, Accuracy: 91.80952310562134\n",
            "Iteration: 300, Loss: 0.24573004245758057, Accuracy: 91.71428680419922\n",
            "Iteration: 300, Loss: 0.24226833879947662, Accuracy: 91.72618985176086\n",
            "Iteration: 300, Loss: 0.24034731090068817, Accuracy: 91.73809289932251\n",
            "Iteration: 300, Loss: 0.237012580037117, Accuracy: 91.88095331192017\n",
            "Iteration: 300, Loss: 0.2367863655090332, Accuracy: 92.0119047164917\n",
            "Iteration: 300, Loss: 0.23529183864593506, Accuracy: 92.03571677207947\n",
            "Iteration: 300, Loss: 0.23300720751285553, Accuracy: 92.08333492279053\n",
            "Iteration: 300, Loss: 0.2326262891292572, Accuracy: 92.0714259147644\n",
            "Iteration: 300, Loss: 0.22783812880516052, Accuracy: 92.2261893749237\n",
            "Iteration: 300, Loss: 0.2249153107404709, Accuracy: 92.23809242248535\n",
            "Iteration: 300, Loss: 0.2204408198595047, Accuracy: 92.11905002593994\n",
            "Iteration: 300, Loss: 0.21619538962841034, Accuracy: 92.2261893749237\n",
            "Iteration: 300, Loss: 0.21564032137393951, Accuracy: 92.14285612106323\n",
            "Iteration: 300, Loss: 0.20905394852161407, Accuracy: 92.14285612106323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyDpgOYtsON8",
        "outputId": "a5445808-a88e-4159-873d-7c635842f6bb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0001"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for images, labels in test_loader:\n",
        "#     images = images.view(-1, input_size, input_size)\n",
        "#     outputs = model(images)\n",
        "#     predictions = torch.max(outputs.data, 1)[1]\n",
        "\n",
        "#     num_images = images.shape[0]\n",
        "#     for image_idx in range(1):\n",
        "#         plt.imshow(images[image_idx])\n",
        "#         plt.axis('off')\n",
        "#         plt.title(predictions[image_idx])\n",
        "#         plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "IwOHyB-iqoP1"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('test.csv')\n",
        "predictions = []\n",
        "image_data = test.loc[:, :].values/255\n",
        "i = 1\n",
        "image_data = torch.tensor(image_data)\n",
        "for image in image_data:\n",
        "  image = image.view(input_size, input_size)\n",
        "  # print(image.shape)\n",
        "  image = torch.tensor(image, dtype=torch.float32)\n",
        "  preds = model(image.unsqueeze(0))\n",
        "  preds = torch.max(preds.data, 1)[1]\n",
        "  predictions.append({'ImageId': i, 'Label': preds[0].item()})\n",
        "  i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVH8LuLwukT-",
        "outputId": "a52558f4-348d-4134-890c-78b23665660f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-a05bb63ca0cb>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  image = torch.tensor(image, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(predictions)\n",
        "df.to_csv('predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "OfLK1ybqze4W"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pcvgsoGf4vcv"
      },
      "execution_count": 68,
      "outputs": []
    }
  ]
}